We have discussed how programming languages allow you to create a high-level description of your program. (If not, please review [[0.c - Programming Languages I - Intro]]). But, much like a human would follow instructions you give them, the computer must understand your instructions to be able to execute them. Even though the programming language forces you to write your program in a structured way, it is still in text. There is still some "special sauce" that is required for a computer to understand how to run your program.

As stated previously, a computer's processor can only run *machine code*, the lowest-level set of instructions available for the processor. All higher-level instructions ultimately must be translated into machine code instructions. 

There are two approaches to facilitate this.

## Compilation

The first approach is *compilation*. Compilation is the process of translating source code into machine code. Therefore, a *compiler* is a program which performs this translation. The output of a compiler is an executable program file (otherwise known as a "binary", because it is no longer a textual file but is instead a blob of instructions based in binary.). The compiler is responsible for checking the meaning of code ("semantics"), checking for any errors or ambiguities ("correctness"), performing the translation, and writing the output as a set of files. 

The compiler, then, is the thing that actually makes a language into executable code. Does this mean that the compiler defines the language? In fact, this is up for debate. Many languages have multiple compilers for them, written by entirely different teams (C++ has MSVC, GCC, LLVM/Clang, and Circle, to name just a few...). Even if a language's specification says that a feature should exist, that feature never actually exists until compilers implement it. Therefore, compiler developers have a lot of say in how a language evolves.

Notably, a compiler doesn't need to be written in the language it compiles. In fact, the first compiler for a given language cannot possibly have been written in that language. You may wonder how the first compilers were built, since this seems like a chicken-and-egg problem. In fact, they were written "by hand" directly as machine code. Once this simple compiler existed, it became possible to write a compiler for a language *in the same language it compiles*. But of course, nothing stops you from writing a C compiler in Python (besides time and effort).

The compiler is designed to generate *machine code* for a specific hardware (i.e. a specific processor) and/or a specific operating system. By default, this is almost always *whatever you are currently running it on*. Some compilers support "cross-compilation", which allows your compiler to target the instruction set of a *different* device or operating system. If this is confusing, you may want to review [[0.d - Low-level Processing Basics]]. 

*"Compile time"* is shorthand for the period of time during program compilation. There are some aspects the compiler can fully predict about your program, i.e. they are "known at compile time". Facts that are inconsistent or incomplete at runtime often cause compiler errors -- for example, if I declare that a function takes either a number or a string, but I only define the function for numbers, my compiler should (rightfully) complain. If it doesn't, then either the compiler has determined that there is no possibility for this error to cause a problem (like if it can be determined that the program will never need to use the function for strings), or something will go **seriously** wrong down the line.

Other types of problems are unknown, and only occur *at run time*. The compiler cannot reason about these situations. Of course, many (most?) situations we need software for require computation to occur at run time, so we cannot always rely on the compiler's powerful checking.

For example, let's say Photoshop wants to add a new algorithm which uses some fancy math groups of nearby pixels. However, for some mathy reason, this algorithm does not define a result if two adjacent pixels are the exact same hue.  The compiler used to create Photoshop has no way of knowing what images you will load into the program, and therefore does not know if this will ever occur. We must explicitly check whether this is true *at run time* (i.e. we must write code to handle this situation when it arises.)

Most compilation is done "Ahead Of Time", or AOT. However, more recently, "Just-in-Time" (JIT) compilation has become popular. In this paradigm, the compiler generates executable code that itself compiles the code when it is needed. This approach can be very fast, because it can generate code optimized for the current hardware and environment, but of course this is slower to start because the code has to be compiled at runtime.

## Interpreters

The second major approach is *interpretation*. In this model, code remains in text form until run time. At run time, the user runs a process which will read the code file(s), *interpret* the textual instructions and then execute the corresponding instructions as it encounters them. Of course, this means that any syntactical parsing needs to happen at runtime. This parsing incurs a performance overhead. The process which interprets the code is called the *interpreter* or the *runtime* for the language. Rarely it is also called a "virtual machine" because the interpreter basically acts as a processor (and memory) for the interpreted language.

In addition, note that the process running the code has not necessarily seen the code before when it is executing. Therefore, if there is an error in the code, this can only be detected at runtime. Often when writing code as a beginner, this is desirable -- it's nice to see your code run a while and only fail at a specific point. However, you may find that this poses a major issue for reliability. You eventually come to appreciate when the compiler warns you ahead of time that something is wrong, and you come to realize that the errors are your friend, not your enemy.

Python is a major popular example of an interpreted language. Sometimes, these languages are called *scripting* languages, although there is no real distinction. Some programs allow users to write their own scripts for the program in order to enable custom functionality. Since this extra functionality, by definition, cannot have been compiled into the program, these scripting languages benefit greatly from not needing to be compiled, hence why interpreted languages have been very popular in this area. (You can find examples to the contrary, though, such as plugins for programs which must be provided as compiled DLLs.)

Remember that Python exists in 2 forms: *the language itself* and the *interpreter runtime*. The Python you download from python.org is the *canonical* (i.e., intended, approved) interpreter, a.k.a CPython, but others exist, such as Cython, PyPy and so on. The CPython runtime is actually written in C -- so, ultimately, all Python code eventually is run as (the compiled form of) C code. Again, the language itself is just a specification for how to write code.

## Bytecode and Intermediate Representation

There is a third approach to all this which takes elements from both approaches.

The way it works is that there is both a compilation step AND an interpreter. The compilation step produces what is called an *intermediate representation* (also sometimes known as "bytecode" because it is just a sequence of bytes). This is your code, compiled to an efficiently-packed set of common instructions which is no longer human-readable, but which is also not directly executable. The interpreter/runtime/VM process then operates on this code to produce the specific machine instructions. Because the interpreter runs later, the compiled bytecode does not target a specific platform. Only the interpreter/runtime/VM is built specifically for the platform, and it interprets the cross-platform compiled bytecode.

Java is perhaps the most well-known language which uses this feature. Java programs, which you may have seen distributed as ".jar" files, are just bundles of bytecode and related data. The JVM (*Java Virtual Machine*) looks through these bundles, finds an entry point, and starts executing the bytecode. The JVM is sort of like a sandboxed environment for the bytecode to execute in. This is why "Java settings" such as maximum heap memory and maximum stack size so often have to be set, as different applications require different amounts of resources but the JVM attempts to control them. In theory, this kind of isolation should also be a security measure, but Java has a long and complicated history with that.

Microsoft, inspired by the Java model, created the .NET platform, which includes a "Common Language Infrastructure" that allows multiple Microsoft-sponsored languages (such as C#, F#, and some forms of C++) to compile to the same bytecode representation (and to use the same libraries). In theory, this would allow great cross-platform, cross-language capability. Microsoft being Microsoft, they tried to paradoxically use this to promote Windows and bungled the situation for a long time. It's only recently that .NET is making progress on this front.

A little known fact is that Python actually compiles your code to bytecode while it runs (this is what .pyc files are). This is an implementation detail that is not important, but it shows how these concepts are mirrored across many languages, even those that don't advertise it.

