A *programming language* is, as the name implies, a language used to program a computer. We call it a language because it resembles any other language, in that it has grammatical and syntactical rules and meaningful symbols which can communicate ideas. Of course, it is a *programming* language because it is sufficiently precise that instructions written in it are explicit enough for a computer to follow. Human languages, on the other hand, tend to be far more ambiguous, often containing implicit meaning.

Sometimes, people ask "why don't we just make a programming language that is basically English?". That is why. Even though English is easy to write and expressive, it doesn't meet the requirement of being explicit and unambiguous.

That's the exact niche that programming languages fill. Pseudocode is basically an intermediate step between the two. "Source code" is a set of instructions written in a programming language. *At some point* (which we will discuss in the next set of notes), this source code has to be translated into instructions for the computer to follow, i.e. become executable. 

Programming languages can be described as "high-level" or "low-level". The exact meaning of this term is relative and has drifted somewhat over the years. But in general, source code is a "higher level", i.e. more abstract, description than the actual instructions that a machine executes. When we write a program, we don't really care where *exactly* things are temporarily stored -- we allow the computer to decide for us because it is of a more concrete, "lower-level" concern. Hence, we don't tell the computer explicitly where to store things temporarily, only that we want things temporarily stored *somewhere*. 

In contrast, if we were directly writing lower-level code, we might explicitly say where to store this data. But because of differences in how each OS/computer/device is set up, this code may not be portable *because it relies on these details*, whereas a higher-level description extends across devices. For example, if I write code that means "store this number on chip A, slot 8"... what happens if my hardware doesn't have a chip A or a slot 8? Our program cannot possibly function correctly.

If we have sufficient knowledge of our hardware, technically, nothing stops us from simply directly writing instructions that the machine understands. These instructions are *machine code* -- direct, binary representations of instructions which the hardware can operate on directly. In fact, the first computer programs were written in machine code. But doing this demands a great deal of knowledge and great effort, all for something that will not necessarily translate to any other hardware. (See the notes on [[0.d - Low-level Processing Basics]] for more details.)

Programming languages were invented to avoid this problem, by allowing us to use a more human-interpretable, higher-level description which is then, at some point, turned into machine code. 

We will discuss this concept in more detail in the next set of notes, [[0.e - Programming Languages II - Compiled vs Interpreted]]. In any case, it is sufficient to say that a programming language is a description of how instructions can be written in a high-level format. The programming language itself *usually* says nothing about how those instructions are acted upon.

## Evolution

It is worth noting that, as a language, a programming language is just a specification. Just as English has evolved since its origin (and emerged semi-synthetically from other earlier languages), programming languages can evolve. When human languages evolve, it is due to a multitude of gradually shifting factors such as pronunciation/accents, the evolution of implicit meanings and new metaphors/idioms, cultural factors, and so on (a course on linguistics could tell you more...). It could be said that *the accepted specification for English has changed*.

When programming languages evolve, it is generally in more discrete steps. For instance, the C programming language spun off of a language called B. C's descendants include D, C++ and Objective-C, among many others. In some respects these are the results of explicit decisions to make a new language with some new design or some new feature set. However, you can also get far more fine-grained in your analysis. C and C++, after a long period of dormancy, started receiving standards updates around the turn of the millenium and have received such standards updates every handful of years since (C99, C++11, C++14, C++17...). Python, now on version 3.11 ever since its inception, has had relatively rapid and eventful development, so much so that some people *who will not be named* are still using Python 2, the last minor version of which (2.7) was released in 2010.

Is each version of a programming language a new language? Each version has different features, different behaviors. Version changes can change the meaning of existing code (most likely, "break it").  

As we said before, developer time is not cheap. Certainly not cheap enough for companies to pay their engineers to fix code every time a language update breaks it. So languages *tend* to make smaller changes and bugfixes in minor or patch versions, and only make these *breaking changes* in major versions which are slow to release (see the [[Semantic Versioning Notes]]).

Some languages adopt other models for their evolution. At the end of the day, though, for a programming language to be useful, it has to produce something that is executable by a computer. Soon, we will see how this comes to happen.



