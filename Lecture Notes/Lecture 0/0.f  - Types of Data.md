If you read the notes on [[0.d - Low-level Processing Basics]], you have seen that binary can be interpreted in multiple ways depending on context.  If we're expecting an unsigned integer, then we know that 0b11111111 is 255.  But I mean... where am I getting this binary *from*, anyway? How do I know it wasn't meant to represent a color or something? A sound? An opcode for a long-lost Commodore 64 variant? Does anything have any meaning at all if it's all just 0s and 1s?

Relax. Yes, when you're looking at binary and hex it can all start to blend together. 
This problem is one big reason we use higher-level languages. When we stop caring about the details and let the compiler or interpreter handle that stuff for us, we can define our programs in terms of meaningful things. If we're feeling frisky, we can even define our own new kinds of things!

The reason we use numbers so often is that they are familiar and simple. We all know the general rules for how numbers work. We are aware of operations that take in one or more numbers and produce another number. But really, numbers are just a type of data. Every individual number is a member of this type, "number". 0 is a number. 1 is a number. 2 is a number. 3.1459 is a number. *e* and *i* can be numbers too, if we define them as such. Sometimes we subdivide the types of number. For example, integers are a specific kind of number (and are included in the "real numbers"). "Complex numbers" generalize even further and include the real numbers themselves. All of these behave the general rules for numbers, but some have their own special rules.
Constructs in math are often some of the simplest types of thing we can imagine (such as sets, sequences, numbers, lines...)

What makes numbers useful is that they have *well-defined operations*. We know what it *means* to add two numbers, and generally, we can find a way to agree on the result. A computer can only execute well-defined operations. On the other hand, "make art" is not well defined. A human can interpret it and do it because they can (and generally *do*) make assumptions, inferences and leaps of logic that a computer would not, and because you, another human, will interpret it as art. A computer, at the lowest level, only does what it is told, and it will not guess what you mean unless it is provided a definition. 

Things that we intuitively understand often lack well-defined operations. If I tell you to "add" two colors, what does that actually mean? "Add" has a meaning that we know, but it's hard to explain it any further than that. It's even harder when it's applied to colors. Does that mean mixing them? Maybe. But when you assume that, you are making a guess at what I mean. It is not exactly verifiable. 

"Semantic" means "relating to meaning". When we discuss the semantics of a program, we are referring to what pieces of it mean in relation to each other, and what they mean to our goals. "Program semantics" rely heavily on this notion that we know when to expect binary data as a certain type of thing. In fact, one of the most important jobs a compiler has is usually to ensure what is called "type correctness". This just means that the compiler is making sure that we, the programmer, have arranged our program in a way where meanings are used consistently.

For example, I might declare that there is a type of thing called a Dog, and that every individual Dog can be fully defined using 32 bits of space. At the underlying level, we know that the Dog is represented as 32 bits on the computer, just like a 32-bit integer. But is it *meaningful* to convert a dog into an integer or vice versa (or, god forbid, to add an integer to a dog)? No, not really. If we attempt to do that without explicitly declaring that we want them to be convertible, the compiler or interpreter will alert us that this operation doesn't exist. Great! Our compiler is verifying the *meaning* of our program according to what we have told it, and will give us feedback on what is needed to make our program's meaning consistent enough to create a well-defined program.

"Primitive" data types are types that are so fundamental that they are seen as useful for almost all programs. Generally, this includes numbers of various kinds, Boolean values (true or false), and characters/strings (like 'a' or 'Bryce'). Often, strings are not actually so primitive, but that's a story for a different time. Indeed, most types we can think of can be modeled using a combination of these, we just might need to get creative. For example, RGB color on your monitor is modeled as a set of 3 integer values, one for red, one for green, and one for blue. These values affect the intensity of the output on any given pixel of your monitor, and these combine in the human eye to present a color.

We also use "collection types" to represent multiples of a given thing. For instance, we might describe a cabinet as being an array of 3 shelves, or we might describe a classroom as a list of students (with unknown length). Collections of a type of thing are actually a part of a type themselves, and can also be operated on.

Much like in math, in programming we often define operations that accept a certain type of data as input and produce *something* as output. Consider the following sexy math text (hope you have LaTeX rendering on!):
$$
f:\mathbb{R}\to \mathbb{R}
$$

That fancy R here stands for the "the set of real numbers". This just means that there is a function *f* which takes some input in the set of real numbers, and produces an output which is again in the set of real numbers. I could just as easily write a function "*g*" which accepts a real number and produces a dog (which, naturally, is just one member of the set of all dogs):
$$
g:\mathbb{R}\to \mathbb{Dog}
$$
The point is this: the operations you define which take a type as input, or which produce a type as output, affect the semantics of what that type actually is. In a certain sense, a program is just a set of mathematical statements about the types of data that you are working with. It's just that, in practice, the data types we like to work with tend to be highly complex.

We will revisit this concept a bit later in our programming journey. Composing new types along with new ways to use data is a fundamental component of software development in general. Determining which operations belong to which data, and bundling them together, is a key concept in the Object-Oriented Programming paradigm. But before we can run, we have to walk.