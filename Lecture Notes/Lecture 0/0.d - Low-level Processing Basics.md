So, just how low-level do these instructions get, anyway? Pretty damn low.

We must briefly discuss binary. A *bit* is a single binary value, 0 or 1, False or True. It is a unit that can only exist in one of these two states at a given time. Strings of these values can represent more complex things (True and True, True and False, False and False, False and True are possible for 2 bits, and so on...).  A system of N bits allows 2^N different states. 

A string of 8 bits is referred to as a byte. A single byte can represent many things, but one common use is to represent a number. 2^8 = 256 different states possibly represented by a byte, so including 0 and assuming negative numbers aren't important, we have a representable range of 0 to 255. If we need negative numbers, then we have to use one bit to indicate sign, so we get -128 to 127 instead (why not -127 to 128? Good question.)

What an individual binary sequence *means* is arbitrary and driven by convention. Sure, 00000000 is 0, 00000001 is 1 and 00000010 is 2... right? Actually, that depends on your system's "endianness" -- which side of the byte means bigger numbers? In the opposite endianness, 00000001 is the value 128. Nothing truly stops me from interpreting 00000001 as 45, or even as "finkeldorf". But I will quickly find that treating it as such makes useful operations much harder, especially if I want them to play nicely with systems made by other people. If I make the assumption that value 11111111 is 128 instead of 127, off-by-one errors may propagate... Or maybe my math will cause my system to underflow to negatives, causing far more severe errors.

Chains of bytes can represent (exponentially) more complex things. A 32-bit unsigned integer contains 32 bits, therefore 4 bytes of 8 bits each, and can represent 2^32 or 4294967296 different values.

Of course, you can represent more than just numbers. ASCII is a convention which gives each possible value for a single byte an interpretation as either a letter of the alphabet (uppercase through lowercase), a digit, some punctuation or special characters, such as the "newline character", often represented as a single backslash followed by n ("\\n"). The value 100, in ASCII, refers to the textual character "d" (lowercase Latin d). Note that the value 6 in ASCII does not correspond to the character "6" (arabic numeral six). This can be confusing.

For more on this idea of interpretation, see the notes on [[0.f  - Types of Data]].

Hexadecimal (or "hex") is an alternative base-16 number system which is often used to represent binary in shorter form. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, and F are valid digits in hexadecimal (and for these purposes, lowercase and uppercase letters are the same). Since this is 16 separate values (2^4), a single hexadecimal digit can represent 4 bits worth of information. So a single byte can be represented by two hexadecimal digits. Hex, binary and numbers can sometimes be difficult to differentiate, so sometimes prefixes are used (0x or h for hex, 0b for binary). 0x00 is h00 is 0b00000000 is 0. 0x0F is h0F is b00001111 is 15. It is worth noting that preceeding 0s are optional for most of these represenations. 0b1111 is 0b00001111. 0x0000BEEF is 0xBEEF.

You don't need to memorize how to translate binary to hex and so forth, but you should be able to recognize what these tend to look like.

## Assembly and Machine Code
You may have heard of "Assembly code". *Assembly* is essentially the lowest level of program description possible before reaching the level of machine code. The reason for this is simple.

Assembly instructions map pretty much one-to-one with the actual bytes that correspond to instructions for the hardware.

Modern processors ingest instructions one at a time. In this context, a single instruction includes both *the operation to be performed* and *operands (i.e. direct inputs) of that operation*. Each of these takes up a specific amount of space.

For example, the 32-bit x86 processor, on which *most* modern processors are based (including its 64-bit descendants) takes at least 1 byte for what is known as an "opcode" and 4 bytes for each operand. (It can take up more space, but that is immaterial for now). The *opcode* is just a byte that corresponds to a specific *hardware-level operation*. For example, the opcode commonly known as "mov" takes two operands. One is a destination (in the form of a *memory address*) and the other is a value to place into that destination. (We'll talk more about memory specifics when we discuss pointers.) Because each operand is a 32-bit value, this instruction allows us to take one 32-bit number value and place it into an location with the address specified by the other 32-bit value. 
Assuming "mov" is represented by the binary 0b00001000, then the instruction for moving the value "2" into a certain memory address might look like the following: 

```hex
0x8 0x2 0xB0BB4451
```
```expanded-hex
0x00000008 0x00000002 0xB0BB4451
```
``` binary
0b00001000 0b00000010 0b10110000101110110100010001010001
```
``` expanded-binary
00001000 00000000 00000000 00000000 00000010 10110000 10111011 01000100 01010001
```
(Try to identify which parts of each segment correspond to the opcode, the value and the destination address.)

If you were somehow able to write the above binary directly into your processor's instruction registers, you could get it to perform that instruction. In reality, of course, a processor is executing an extremely large amount of these instructions every second. Moving a number into a specific place on a chip, by itself, does nothing for us. 

Assembly, as stated, is a mapping of those sequences above to something slightly more human readable. The above example would, in the "Intel" style/syntax of assembly, look like:

``` assembly-Intel
mov 0xB0BB4451, 2
```

Notice that the order has changed. Remember, the way that an instruction is represented on your screen is just convention. An *assembler* is responsible for turning the assembly you see above into the appropriate byte representations in the right order for the processor to understand it. 

A *disassembler* takes machine code and *renders it into assembly*. As there are multiple syntaxes of assembly language (AT&T and Intel are just a few), a disassembler could produce any of them from the same machine code and they would all be correct representations of the same thing.

Actually writing assemblers and disassemblers is very difficult and requires you to literally have a reference manual on hand for the processor in question. We stand on the shoulders of giants who did it so that we don't have to.

The set of instructions which a processor understands is known as (perhaps obviously) its *instruction set*. The instruction set defines which opcodes exist, what they mean, and how big the operands are (i.e., how the processor should interpret the data that follows the opcode).

As time allows, we will revisit assembly later.

