Yeah, I know, you don't care. Just pay attention for a minute. We will briefly catch up to the modern day and get to real-world stuff.

Computers were described long before they were actually built.

Generally speaking, a "computer" in the abstract sense is any mechanism which can find an output given some input. Alan Turing first described the abstract "Turing Machine". This mechanism operated on a single, linear piece of tape containing segments labeled 0 or 1. He also proved that by defining just a few capabilities of the Turing Machine, it would be able, in principle, to perform any possible computational task. Hence, when we talk about any system capable of producing output from some input, we can describe it as "Turing Complete" (i.e., capable in principle of performing any computational task) or "Turing Incomplete" (where the computational tasks that are possible to complete in the system are limited in some way).

But actually, we use that term somewhat loosely. Technically, no system is Turing complete unless it has infinite memory. But by changing that just to "enough memory to perform the computation", we get a functionally useful definition of "Turing complete".

By defining certain things as input and certain things as output, a surprising amount of systems you might not expect can be proven to be Turing complete with few assumptions.  Redstone in Minecraft is Turing complete. Magic the Gathering is Turing complete.

How exactly the abstract Turing machine worked is not entirely important for us. It is sufficient to say that it worked using a single, infinite linear structure ("tape") composed of segments containing 0 or 1 (*binary* values). The tape's state informed the behavior of the machine, such that the initial state was responsible for determining subsequent states in a deterministic way. After each step, the machine would check the current state and take the corresponding next step. One such "step" is "do nothing". Hence, the machine could enter a state in which no more changes to state would occur, at which point the current state becomes final. 

... But how would you know when the final state would occur? The only way to know *when*, or even *if* the final state will occur, is to actually run through the process. Hence, it is impossible to tell if a running program will stop *until it stops*. This is also known as "the halting problem". Perhaps you think, "but wait, can't I simply look at the program, find the next steps, and be able to deduce whether it will stop or not?". Yes. But in that case, it is *your mind* running the rest of the program, not the computer. You can't tell whether the program will stop until you have, at least once, run through the program in your mind. *This itself is computation.*

Early computer researchers were very interested in turning the abstract description provided by Turing into a real system. Of course, some concessions and alterations were made along the way in order to create a system that would work. You will see this idea echoed throughout computer history. For example, artificial neural networks, just now seeing really strong use in artificial intelligence, were first described in the 60s. The concept was developed along the way, with several "AI winters" in between, where technological development stagnated.

Eventually, we settled on (real) computer systems with several core components:
- a central processing unit, to run instructions
- a volatile memory unit, to store data temporarily 
- a nonvolatile storage unit, for persistent long-term storage
- a motherboard, to host the above components and allow them to communicate

## What is a program?

A "program" is a set of instructions that operate on some input and (possibly) produce some output. Note that this description includes not just computer programs which are tightly specified, but also verbal instructions, such as those your boss gives you at work. Any defined "business process" can be described as a program.

In the Turing machine, the program is a portion of the initial state, and the input is another portion of the input state, with the output being a designated portion of the final state (if any exists). Very notably, in the Turing description, the only clear demarcation of which parts of the tape are input/output and which are instructions is that provided by humans. In fact, portions of tape that are used for instructions can be overwritten and read as data (or vice versa, data overwritten and used as instructions).

In practice, it is generally useful to think about programs as consisting of two types of thing: operations and data. Operations do something on data. For instance, I may define an operation "Add" which takes two numbers and produces their sum. "Add" is an operation, the relevant data is the two numbers I provide. We might say that "Add(5, 6)" is a program consisting of one operation and its inputs. This "combined object", an operation with its inputs provided, can be referred to as a single "instruction". So my program in this case is composed of one instruction, and it produces 11 as output. 

It is through the composition of multiple instructions that programs become larger, more complex and (sometimes) more useful. For instance, if I want a program that can add any two numbers I provide, we might specify "Add(x, y)" where x and y are whatever our inputs are. This is suddenly far more flexible than "Add(5, 6)". Parametrizing instructions in this way is often very useful. My program now lets me add any two numbers and get the result. Note that we have not yet defined what it means to add two numbers. Rather, we *know that an implementation exists* (in our heads) for the "Add" operation. We make the assumption that whoever is reading our program will be able to access/run this implementation when following our instructions. If not, then it makes sense to explicitly define the implementation.

Think about it: How do you add any two numbers? Can you write down your process on paper? Try to be as precise as possible and assume I know absolutely nothing. (This is part of Assignment 1 -- try to do this now.)

As you may have discovered during that process, it is often necessary, or at least helpful, to separate out some parts of your process into separate chunks of operations. We can define a new operation, Multiply(x, y), which relies upon Add(x, y). For example:

```
define Multiply(x, y) as:
    set z to 0
	for each number from 1 through y:
		set z to the result of Add(z, x)
	output z
```

This is an example of *pseudocode*. Pseudocode is a form of writing that is meant to convey instructions precisely, without actually needing to be computer-executable or computer-interpretable. 

Note that in our definition of Multiply, we never actually say how to Add. Instead, we rely on the fact that either 1. the definition of Add is available somewhere else in our program, or 2. that the person using our instructions already knows how to Add. These ideas will be echoed later when we talk about modularization and libraries.

Now that we have these operations defined, we can write a more complex program which composes them. This type of composition is the core essence of software development.




